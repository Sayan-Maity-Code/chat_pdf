{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go to app.py to run the code . These are all trial & error codes , for future references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv # pip install python-dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "parser_key = os.getenv(\"llama_parser\")#It only parse 1000 pages per day for free or you will have to upgrade the plan ðŸ¤‘ðŸ’¸\n",
    "'''https://cloud.llamaindex.ai/api-key'''#and there must be 1200 pages per file max\n",
    "pinecone_key=os.getenv(\"pinecone_api_key\")\n",
    "huggingface_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if parser_key:\n",
    "    print(True)\n",
    "if pinecone_key:\n",
    "    print(True)\n",
    "if huggingface_key:\n",
    "    print(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 28b9e3df-ec31-4130-abd9-36cbd7358cc7\n",
      "# Journal of Physics: Conference Series\n",
      "\n",
      "PAPER &#8226; OPEN ACCESS\n",
      "\n",
      "Research on Machine Learning and Its Algorithms and Development\n",
      "\n",
      "To cite this article: Wei Jin 2020 J. Phys.: Conf. Ser. 1544 012003\n",
      "\n",
      "View the article online for updates and enhancements.\n",
      "\n",
      "IOP ebooks\n",
      "\n",
      "Bringing together innovative digital publishing with leading authors from the global scientific community.\n",
      "\n",
      "Start exploring the collection-download the first chapter of every title for free.\n",
      "\n",
      "This content was downloaded from IP address 158.46.154.149 on 03/06/2020 at 13:35\n",
      "# Research on Machine Learning and Its Algorithms and Development\n",
      "\n",
      "Wei Jin\n",
      "\n",
      "Northwestern Polytechnical University Ming De College, Xiâ€™an, Shaanxi, China\n",
      "\n",
      "Abstract: This article analyzes the basic classification of machine learning, including supervised learning, unsupervised learning, and reinforcement learning. It combines analysis on common algorithms in machine learning, such as decision tree algorithm, random forest algorithm, artificial neural network algorithm, SVM algorithm, Boosting and Bagging algorithm, BP algorithm. Through the development of theoretical systems, further improvement of autonomous learning capabilities, the integration of multiple digital technologies, and the promotion of personalized custom services, the purpose is to improve people's awareness of machine learning and accelerate the speed of popularization of machine learning.\n",
      "\n",
      "# 1. Introduction\n",
      "\n",
      "With the rapid development of science and technology, artificial intelligence has also ushered in new developmen\n",
      "# ICSP 2020\n",
      "\n",
      "# IOP Publishing Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "process, but rely on the machine itself to complete the analysis of data information. In practice, the operation method is to let the machine learn the basic concepts and content, and then give the machine enough freedom to complete a series of content learning, including concepts and content similar to the basic principles, such as tree roots. In general, the continuous improvement of learning in stages has increased the breadth of machine learning content. At present, unsupervised learning includes algorithms such as deep belief networks and autoencoders. Such situations are conducive to the solution of clustering problems and have good applications in the development of many industries [2].\n",
      "\n",
      "# Reinforcement Learning\n",
      "\n",
      "In addition to supervised learning and unsupervised learning, there are also application methods of reinforcement learning in machine learning. Th\n",
      "# 3.3. Artificial Neural Network Algorithm\n",
      "\n",
      "The so-called artificial neural network refers to imitating the process of human information transmission, classifying different data into one neuron, and connecting the data neurons with the help of the Internet to achieve complex memory activities. However, the artificial neural network algorithm is based on this unfolding data analysis process. Among the delineated neurons, each digital unit has a high degree of authenticity, and the data can complete the process of external output. It's just like the human body moves forward, stops, and runs. In the artificial neural network algorithm, the data information presented has a variety of application characteristics, and the corresponding analysis process can be completed according to actual needs. At present, commonly used artificial neural networks include multilayer forward neural networks MLFN, self-organizing neural networks, SOM, and ART [5]. In order to facilitate the analysis and calcul\n",
      "ICSP 2020\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "|Hidden Layer|Expected Output|\n",
      "|---|---|\n",
      "|Input Layer|Output Layer|\n",
      "|Layer4|Layer Lz|\n",
      "\n",
      "Figure 1 Basic Principles of Algorithm Application\n",
      "\n",
      "As can be seen from the above figure, the direction of the information flow of forward propagation is input=(layer â†’ hidden layer â†’ output layer, and its mathematical model is: hz: Wx- b). Where Wi and b are their weights and bias parameters, f (W, b; x): R â†’ R is called the excitation function, and sigmoid can be selected in practical applications, Tanh, ReLU and other functions or their variants, hWï¼Œb(x) are the network output values. In practical applications, the BP algorithm can be implemented by the steepest descent method, Newton method and its improved algorithm, quasi-Newton method and its correction algorithm, etc. At present, the L-BFGS algorithm is most widely used, and non-precise line search methods are often used to complete the op\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "Internet technology, the autonomous learning ability of machines will be further strengthened. Whether it is supervised learning or unsupervised learning, the autonomy that machine learning can master will continue to increase. In the future learning process of the machine, the machine will perform targeted or extensive learning according to its own needs, which also reduces the economic cost of the enterprise to update the equipment structure, thereby laying a solid foundation for the stable development of the enterprise economy.\n",
      "\n",
      "# Integration of Multiple Digital Technologies\n",
      "\n",
      "At this stage, relying on Internet technology has produced many branch technologies, such as Internet of Things technology, digital technology, cloud computing technology, etc. These technologies can provide many convenient conditions in the process of data calculation. Although these digita\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "\n",
    "# Apply nest_asyncio to handle asynchronous operations\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize LlamaParse with API key from environment variable\n",
    "api_key = parser_key  # Ensure your API key is set here or in the environment variables\n",
    "parser = LlamaParse(\n",
    "    api_key=api_key,\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Function to handle loading of documents synchronously\n",
    "def extract_documents_sync(path):\n",
    "    if os.path.isfile(path):\n",
    "        return parser.load_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return parser.load_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Function to handle loading of documents asynchronously\n",
    "async def extract_documents_async(path):\n",
    "    if os.path.isfile(path):\n",
    "        return await parser.aload_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return await parser.aload_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Path to the file or directory containing the documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "\n",
    "# Extract and print the content of the documents synchronously\n",
    "documents = extract_documents_sync(path)\n",
    "for doc in documents:\n",
    "    print(doc.text[:1000])# Print the first 1000 characters of each \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 58f333d4-4132-4586-b981-7c527e39bb2e\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Create and save FAISS index\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m db\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Load the saved index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1056\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1045\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m        VectorStore: VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1056\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m   1057\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Document' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "#pip install tf-keras\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LlamaParse\n",
    "parser_key = os.getenv(\"llama_parser\")\n",
    "parser = LlamaParse(api_key=parser_key, result_type=\"markdown\", num_workers=4, verbose=True)\n",
    "\n",
    "# Function to extract documents\n",
    "def extract_documents(path):\n",
    "    if os.path.isfile(path):\n",
    "        return parser.load_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return parser.load_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Extract documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "documents = extract_documents(path)\n",
    "\n",
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create and save FAISS index\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "# Load the saved index\n",
    "loaded_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "# Initialize Hugging Face pipeline for language model\n",
    "model_name = \"google/flan-t5-large\"  # You can change this to a different model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Set up retriever with contextual compression\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=loaded_db.as_retriever(search_kwargs={\"k\": 4})\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Function to answer questions\n",
    "def answer_question(query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n",
    "\n",
    "# Example usage\n",
    "query = \"How many machine learning algorithms are there?\"\n",
    "answer, source_docs = answer_question(query)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in source_docs:\n",
    "    print(f\"- {doc.metadata['source']}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install langchain langchain-huggingface faiss-cpu transformers torch dotenv llama-parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 07cdd29f-4ddc-4d0b-859e-4e78ee0e9e80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LlamaParse\n",
    "parser_key = os.getenv(\"llama_parser\")\n",
    "parser = LlamaParse(api_key=parser_key, result_type=\"markdown\", num_workers=4, verbose=True)\n",
    "\n",
    "# Function to extract documents\n",
    "def extract_documents(path):\n",
    "    if os.path.isfile(path):\n",
    "        return parser.load_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return parser.load_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Extract documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "documents = extract_documents(path)\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "texts = []\n",
    "metadatas = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    texts.extend(chunks)\n",
    "    metadatas.extend([doc.metadata] * len(chunks))\n",
    "\n",
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create and save FAISS index\n",
    "db = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "# Load the saved index\n",
    "loaded_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you summerize the topic? \n",
      "\n",
      "Answer: The theoretical system continues to mature process, but rely on the machine itself to complete the analysis of data information. In practice, the operation method is to let the machine learn the basic concepts and content, and then give the machine enough freedom to complete a series of content learning, including concepts and content similar to the basic principles, such as tree roots Promotion of Personalized Customization Services.\n",
      "\n",
      "Source Documents:\n",
      "- : # 1. Introduction...\n",
      "- : machine learning theory will be continuously strengthened, and the degree of refinement of the conte...\n",
      "- : process presents a certain regularity, and the learning content is more systematic [1]...\n",
      "- : # 4. Research on Machine Learning Development # 4.1. Theoretical System Continues to Mature...\n",
      "- : process, but rely on the machine itself to complete the analysis of data information. In practice, t...\n",
      "- : Promotion of Personalized Customization Services...\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code remains the same)\n",
    "\n",
    "# Initialize Hugging Face pipeline for language model\n",
    "model_name = \"google/flan-t5-large\"  # Upgraded to a larger model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create a more detailed prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Provide a detailed explanation in your answer, using up to five sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Detailed Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Set up retriever with contextual compression\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=loaded_db.as_retriever(search_kwargs={\"k\": 6})  # Increased from 4 to 6\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Function to answer questions\n",
    "def answer_question(query):\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n",
    "\n",
    "# Example usage\n",
    "query = \"Can you summerize the topic?\"\n",
    "answer, source_docs = answer_question(query)\n",
    "print(query,\"\\n\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in source_docs:\n",
    "    print(f\"- : {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 28bd0b85-5696-4ae2-b668-27ba0ac01195\n",
      "{'id_': '2b05c1a6-8fa3-4273-b168-e2bdabada1a6', 'embedding': None, 'metadata': {}, 'excluded_embed_metadata_keys': [], 'excluded_llm_metadata_keys': [], 'relationships': {}, 'text': '# Journal of Physics: Conference Series\\n\\nPAPER &#8226; OPEN ACCESS\\n\\nResearch on Machine Learning and Its Algorithms and Development\\n\\nTo cite this article: Wei Jin 2020 J. Phys.: Conf. Ser. 1544 012003\\n\\nView the article online for updates and enhancements.\\n\\nIOP ebooks\\n\\nBringing together innovative digital publishing with leading authors from the global scientific community.\\n\\nStart exploring the collection-download the first chapter of every title for free.\\n\\nThis content was downloaded from IP address 158.46.154.149 on 03/06/2020 at 13:35', 'mimetype': 'text/plain', 'start_char_idx': None, 'end_char_idx': None, 'text_template': '{metadata_str}\\n\\n{content}', 'metadata_template': '{key}: {value}', 'metadata_seperator': '\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Extract documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "documents = extract_documents(path)\n",
    "\n",
    "# Inspect the first document to understand its structure\n",
    "first_document = documents[0]\n",
    "print(first_document.__dict__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os# It is used to interacting with our os , we used it to fetch environment variables\n",
    "import shutil# is used to handle file operations related to the FAISS index directory. Specifically, it ensures that any existing FAISS index directory is deleted before creating a new one.\n",
    "import gradio as gr# is for the ui\n",
    "from dotenv import load_dotenv#to load env_variables from .env file\n",
    "from llama_parse import LlamaParse#(is a document parsing tool) It allows the system to process and analyze document content, which is essential for further operations like indexing, summarization, and question-answering.\n",
    "from langchain_community.vectorstores import FAISS#(Facebook AI Similarity Search),(for efficient similarity search and clustering of dense vectors),It enables the system to quickly find and retrieve relevant document sections based on query similarity, which is crucial for effective information retrieval.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline#Embeddings capture semantic meaning, allowing the system to compare and search texts based on their content, rather than just keyword matching,Pipeline provides a simple interface to various NLP tasks using pre-trained models.\n",
    "from langchain.chains import RetrievalQA# It integrates retrieval mechanisms and language models to provide accurate and contextually relevant answers\n",
    "from langchain.prompts import PromptTemplate# helps the model understand the input structure and respond appropriately.\n",
    "from langchain.retrievers import ContextualCompressionRetriever#It retrieves and compresses document content, making it suitable for models with input length limitations.\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor#It extracts essential parts of the document to help answer specific queries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline#AutoTokenizer is used to tokenize the input text, breaking it down into tokens that the model can understand.\n",
    "#AutoModelForSeq2SeqLM is a pre-trained sequence-to-sequence language model used for generating text based on the input tokens.\n",
    "#it's used to create a text generation pipeline for generating answers or summaries.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter#This tool splits long texts into smaller, manageable chunks.\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Initialize LlamaParse\n",
    "parser_key = os.getenv(\"llama_parser\")  # Fetches the LlamaParse API key from environment variables.\n",
    "#It only parse 1000 pages per day for free or you will have to upgrade the plan ðŸ¤‘ðŸ’¸\n",
    "'''https://cloud.llamaindex.ai/api-key'''#and there must be 1200 pages per file max\n",
    "parser = LlamaParse(api_key=parser_key, result_type=\"markdown\", num_workers=4, verbose=True)  # Initializes LlamaParse with the API key and specific settings.\n",
    "# Using markdown makes the output easier to read and understand.\n",
    "#num_workers=4, the parsing task can be distributed across four workers, allowing for parallel processing.\n",
    "#When verbose is set to True, the parser provides detailed logging information about the parsing process.\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "loaded_db = None  # Will hold the loaded FAISS database.\n",
    "qa_chain = None  # Will hold the RetrievalQA chain.\n",
    "document_summary = \"\"  # Will hold the document summary.\n",
    "tokenizer = None  # Will hold the tokenizer for the language model.\n",
    "model = None  # Will hold the language model.\n",
    "\n",
    "def extract_documents(file):\n",
    "    return parser.load_data(file.name)## Uses LlamaParse to extract text from the uploaded file.\n",
    "\n",
    "def process_document(file):\n",
    "    global loaded_db, qa_chain, document_summary, tokenizer, model# Declare global variables to modify them from anywhere.\n",
    "\n",
    "    documents = extract_documents(file)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,  # Maximum size of each text chunk.\n",
    "        chunk_overlap=20,  # Overlap between chunks to maintain context.\n",
    "        length_function=len,  # Function to determine chunk length.\n",
    "    )\n",
    "\n",
    "    texts = []  # List to store text chunks.\n",
    "    metadatas = []  # List to store metadata for each chunk.\n",
    "    for doc in documents:  # Loop through each extracted document.\n",
    "        chunks = text_splitter.split_text(doc.text)  # Split document text into chunks.\n",
    "        texts.extend(chunks)  # Add chunks to the texts list.\n",
    "        metadatas.extend([doc.metadata] * len(chunks))  # Add metadata for each chunk.\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # Initialize opensource embeddings model.\n",
    "\n",
    "    if os.path.exists(\"faiss_index\"):  # Check if FAISS index directory exists.\n",
    "        shutil.rmtree(\"faiss_index\")  # Remove existing FAISS index directory.\n",
    "    db = FAISS.from_texts(texts, embeddings, metadatas=metadatas)  # Create FAISS index from texts and embeddings.\n",
    "    db.save_local(\"faiss_index\")  # Save the FAISS index locally.\n",
    "    loaded_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)  # Load the saved FAISS index.It tells the FAISS library to skip some of the safety checks that are typically enforced during deserialization.\n",
    "\n",
    "\n",
    "    model_name = \"google/flan-t5-large\"  # Specify the language model to use.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # Initialize the tokenizer for the language model.\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  # Initialize the language model.\n",
    "\n",
    "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)  # Create a text generation pipeline.\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)  # Initialize HuggingFacePipeline with the text generation pipeline.\n",
    "\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Provide a detailed explanation in your answer, using up to five sentences.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Detailed Answer:\"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])# Initialize the prompt template.\n",
    "\n",
    "    compressor = LLMChainExtractor.from_llm(llm)  # Initialize the document compressor.\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,  # Set the document compressor.\n",
    "        base_retriever=loaded_db.as_retriever(search_kwargs={\"k\": 4})  #loaded_db.as_retriever(): This method converts the FAISS index (loaded_db) into a retriever object that can be used to perform similarity searches.{\"k\": 4} indicates that the retriever should return the top 4 most similar documents or text chunks based on the similarity score.\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,  # Set the language model.\n",
    "        chain_type=\"stuff\",  #Choosing the Right Chain Type\n",
    "        #\"stuff\": Simple and direct, good for straightforward contexts.\n",
    "        #\"map_reduce\": Best for complex or large documents requiring summarization.\n",
    "        #\"refine\": Suitable for detailed or nuanced answers requiring refinement.\n",
    "        #\"qa\": Optimized for direct question-answering tasks.\n",
    "        retriever=compression_retriever,  #retriever=compression_retriever: Configures the QA chain to use a retriever that not only fetches relevant documents but also compresses them to ensure the input to the language model is concise and effective.\n",
    "        return_source_documents=True,  # Indicate to return source documents.\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}  # Set additional chain type parameters.\n",
    "    )\n",
    "\n",
    "    document_summary = summarize_document(loaded_db)  # Generate a summary of the document.\n",
    "\n",
    "    return \"Document processed successfully. You can now ask questions or view the summary.\"  # Return a success message.\n",
    "\n",
    "def summarize_document(db, chunk_size=400):\n",
    "    all_docs = db.similarity_search(\"\", k=db.index.ntotal)  # Retrieve all documents from the database.\n",
    "    full_text = \" \".join(doc.page_content for doc in all_docs)  # Combine all document texts.\n",
    "    chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]  # Split the full text into chunks.\n",
    "    return \" \".join(chunks)  # Return the combined chunks as the summary.\n",
    "\n",
    "def answer_question(query):\n",
    "    if not query.strip():  # Check if the query is empty.\n",
    "        return \"Please enter a question before clicking the Ask button.\"\n",
    "    \n",
    "    if not loaded_db or not qa_chain:  # Check if the database or QA chain is not initialized.\n",
    "        return \"Please process a document first.\"\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": query})  # Get the answer from the QA chain.\n",
    "    return result[\"result\"].strip().rstrip(\"[1]\").strip()  # Clean up and return the answer.\n",
    "\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks(\".gradio-container {background: url(file='./img.jpeg')}\") as demo:  # Create a Gradio interface with custom CSS.\n",
    "    gr.Markdown(\"# Intelligent Document Q&A System\")  # Add a title.\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document\")  # Add a file upload input.\n",
    "        process_button = gr.Button(\"Process Document\", elem_classes=[\"gradio-button\"])  # Add a button to process the document.\n",
    "\n",
    "    status_output = gr.Textbox(label=\"Status\", elem_classes=[\"gradio-textbox\"])  # Add a textbox for status messages.\n",
    "\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Ask a question\", elem_classes=[\"gradio-textbox\"])  # Add a textbox for inputting questions.\n",
    "        answer_output = gr.Textbox(label=\"Answer\", elem_classes=[\"gradio-textbox\"])  # Add a textbox for displaying answers.\n",
    "\n",
    "    ask_button = gr.Button(\"Ask\", elem_classes=[\"gradio-button\"])  # Add a button to ask a question.\n",
    "\n",
    "    summary_output = gr.Textbox(label=\"Document Summary\", lines=10, elem_classes=[\"gradio-textbox\"])  # Add a textbox for the document summary.\n",
    "    summary_button = gr.Button(\"Generate Summary\", elem_classes=[\"gradio-button\"])  # Add a button to generate the summary.\n",
    "\n",
    "    process_button.click(process_document, inputs=[file_input], outputs=[status_output])  # Define the click action for the process button.\n",
    "    ask_button.click(answer_question, inputs=[question_input], outputs=[answer_output])  # Define the click action for the ask button.\n",
    "    summary_button.click(lambda: document_summary, outputs=[summary_output])  # Define the click action for the summary button.\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(share=True)  # Launch the Gradio interface with sharing enabled.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
