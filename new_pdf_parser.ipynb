{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv # pip install python-dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "parser_key = os.getenv(\"llama_parser\")#It only parse 1000 pages per day for free or you will have to upgrade the plan ðŸ¤‘ðŸ’¸\n",
    "'''https://cloud.llamaindex.ai/api-key'''#and there must be 1200 pages per file max\n",
    "pinecone_key=os.getenv(\"pinecone_api_key\")\n",
    "huggingface_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "if parser_key:\n",
    "    print(True)\n",
    "if pinecone_key:\n",
    "    print(True)\n",
    "if huggingface_key:\n",
    "    print(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 28b9e3df-ec31-4130-abd9-36cbd7358cc7\n",
      "# Journal of Physics: Conference Series\n",
      "\n",
      "PAPER &#8226; OPEN ACCESS\n",
      "\n",
      "Research on Machine Learning and Its Algorithms and Development\n",
      "\n",
      "To cite this article: Wei Jin 2020 J. Phys.: Conf. Ser. 1544 012003\n",
      "\n",
      "View the article online for updates and enhancements.\n",
      "\n",
      "IOP ebooks\n",
      "\n",
      "Bringing together innovative digital publishing with leading authors from the global scientific community.\n",
      "\n",
      "Start exploring the collection-download the first chapter of every title for free.\n",
      "\n",
      "This content was downloaded from IP address 158.46.154.149 on 03/06/2020 at 13:35\n",
      "# Research on Machine Learning and Its Algorithms and Development\n",
      "\n",
      "Wei Jin\n",
      "\n",
      "Northwestern Polytechnical University Ming De College, Xiâ€™an, Shaanxi, China\n",
      "\n",
      "Abstract: This article analyzes the basic classification of machine learning, including supervised learning, unsupervised learning, and reinforcement learning. It combines analysis on common algorithms in machine learning, such as decision tree algorithm, random forest algorithm, artificial neural network algorithm, SVM algorithm, Boosting and Bagging algorithm, BP algorithm. Through the development of theoretical systems, further improvement of autonomous learning capabilities, the integration of multiple digital technologies, and the promotion of personalized custom services, the purpose is to improve people's awareness of machine learning and accelerate the speed of popularization of machine learning.\n",
      "\n",
      "# 1. Introduction\n",
      "\n",
      "With the rapid development of science and technology, artificial intelligence has also ushered in new developmen\n",
      "# ICSP 2020\n",
      "\n",
      "# IOP Publishing Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "process, but rely on the machine itself to complete the analysis of data information. In practice, the operation method is to let the machine learn the basic concepts and content, and then give the machine enough freedom to complete a series of content learning, including concepts and content similar to the basic principles, such as tree roots. In general, the continuous improvement of learning in stages has increased the breadth of machine learning content. At present, unsupervised learning includes algorithms such as deep belief networks and autoencoders. Such situations are conducive to the solution of clustering problems and have good applications in the development of many industries [2].\n",
      "\n",
      "# Reinforcement Learning\n",
      "\n",
      "In addition to supervised learning and unsupervised learning, there are also application methods of reinforcement learning in machine learning. Th\n",
      "# 3.3. Artificial Neural Network Algorithm\n",
      "\n",
      "The so-called artificial neural network refers to imitating the process of human information transmission, classifying different data into one neuron, and connecting the data neurons with the help of the Internet to achieve complex memory activities. However, the artificial neural network algorithm is based on this unfolding data analysis process. Among the delineated neurons, each digital unit has a high degree of authenticity, and the data can complete the process of external output. It's just like the human body moves forward, stops, and runs. In the artificial neural network algorithm, the data information presented has a variety of application characteristics, and the corresponding analysis process can be completed according to actual needs. At present, commonly used artificial neural networks include multilayer forward neural networks MLFN, self-organizing neural networks, SOM, and ART [5]. In order to facilitate the analysis and calcul\n",
      "ICSP 2020\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "|Hidden Layer|Expected Output|\n",
      "|---|---|\n",
      "|Input Layer|Output Layer|\n",
      "|Layer4|Layer Lz|\n",
      "\n",
      "Figure 1 Basic Principles of Algorithm Application\n",
      "\n",
      "As can be seen from the above figure, the direction of the information flow of forward propagation is input=(layer â†’ hidden layer â†’ output layer, and its mathematical model is: hz: Wx- b). Where Wi and b are their weights and bias parameters, f (W, b; x): R â†’ R is called the excitation function, and sigmoid can be selected in practical applications, Tanh, ReLU and other functions or their variants, hWï¼Œb(x) are the network output values. In practical applications, the BP algorithm can be implemented by the steepest descent method, Newton method and its improved algorithm, quasi-Newton method and its correction algorithm, etc. At present, the L-BFGS algorithm is most widely used, and non-precise line search methods are often used to complete the op\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "Internet technology, the autonomous learning ability of machines will be further strengthened. Whether it is supervised learning or unsupervised learning, the autonomy that machine learning can master will continue to increase. In the future learning process of the machine, the machine will perform targeted or extensive learning according to its own needs, which also reduces the economic cost of the enterprise to update the equipment structure, thereby laying a solid foundation for the stable development of the enterprise economy.\n",
      "\n",
      "# Integration of Multiple Digital Technologies\n",
      "\n",
      "At this stage, relying on Internet technology has produced many branch technologies, such as Internet of Things technology, digital technology, cloud computing technology, etc. These technologies can provide many convenient conditions in the process of data calculation. Although these digita\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "\n",
    "# Apply nest_asyncio to handle asynchronous operations\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize LlamaParse with API key from environment variable\n",
    "api_key = parser_key  # Ensure your API key is set here or in the environment variables\n",
    "parser = LlamaParse(\n",
    "    api_key=api_key,\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Function to handle loading of documents synchronously\n",
    "def extract_documents_sync(path):\n",
    "    if os.path.isfile(path):\n",
    "        return parser.load_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return parser.load_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Function to handle loading of documents asynchronously\n",
    "async def extract_documents_async(path):\n",
    "    if os.path.isfile(path):\n",
    "        return await parser.aload_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return await parser.aload_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Path to the file or directory containing the documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "\n",
    "# Extract and print the content of the documents synchronously\n",
    "documents = extract_documents_sync(path)\n",
    "for doc in documents:\n",
    "    print(doc.text[:1000])# Print the first 1000 characters of each \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 58f333d4-4132-4586-b981-7c527e39bb2e\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Create and save FAISS index\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m db\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Load the saved index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1056\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1045\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m        VectorStore: VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1056\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m   1057\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Document' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "#pip install tf-keras\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LlamaParse\n",
    "parser_key = os.getenv(\"llama_parser\")\n",
    "parser = LlamaParse(api_key=parser_key, result_type=\"markdown\", num_workers=4, verbose=True)\n",
    "\n",
    "# Function to extract documents\n",
    "def extract_documents(path):\n",
    "    if os.path.isfile(path):\n",
    "        return parser.load_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return parser.load_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Extract documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "documents = extract_documents(path)\n",
    "\n",
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create and save FAISS index\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "# Load the saved index\n",
    "loaded_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "# Initialize Hugging Face pipeline for language model\n",
    "model_name = \"google/flan-t5-large\"  # You can change this to a different model if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Set up retriever with contextual compression\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=loaded_db.as_retriever(search_kwargs={\"k\": 4})\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Function to answer questions\n",
    "def answer_question(query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n",
    "\n",
    "# Example usage\n",
    "query = \"How many machine learning algorithms are there?\"\n",
    "answer, source_docs = answer_question(query)\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in source_docs:\n",
    "    print(f\"- {doc.metadata['source']}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install langchain langchain-huggingface faiss-cpu transformers torch dotenv llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 07cdd29f-4ddc-4d0b-859e-4e78ee0e9e80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LlamaParse\n",
    "parser_key = os.getenv(\"llama_parser\")\n",
    "parser = LlamaParse(api_key=parser_key, result_type=\"markdown\", num_workers=4, verbose=True)\n",
    "\n",
    "# Function to extract documents\n",
    "def extract_documents(path):\n",
    "    if os.path.isfile(path):\n",
    "        return parser.load_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return parser.load_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Extract documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "documents = extract_documents(path)\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "texts = []\n",
    "metadatas = []\n",
    "for doc in documents:\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    texts.extend(chunks)\n",
    "    metadatas.extend([doc.metadata] * len(chunks))\n",
    "\n",
    "# Initialize Hugging Face embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create and save FAISS index\n",
    "db = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "# Load the saved index\n",
    "loaded_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you summerize the topic? \n",
      "\n",
      "Answer: The theoretical system continues to mature process, but rely on the machine itself to complete the analysis of data information. In practice, the operation method is to let the machine learn the basic concepts and content, and then give the machine enough freedom to complete a series of content learning, including concepts and content similar to the basic principles, such as tree roots Promotion of Personalized Customization Services.\n",
      "\n",
      "Source Documents:\n",
      "- : # 1. Introduction...\n",
      "- : machine learning theory will be continuously strengthened, and the degree of refinement of the conte...\n",
      "- : process presents a certain regularity, and the learning content is more systematic [1]...\n",
      "- : # 4. Research on Machine Learning Development # 4.1. Theoretical System Continues to Mature...\n",
      "- : process, but rely on the machine itself to complete the analysis of data information. In practice, t...\n",
      "- : Promotion of Personalized Customization Services...\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code remains the same)\n",
    "\n",
    "# Initialize Hugging Face pipeline for language model\n",
    "model_name = \"google/flan-t5-large\"  # Upgraded to a larger model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create a more detailed prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Provide a detailed explanation in your answer, using up to five sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Detailed Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Set up retriever with contextual compression\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=loaded_db.as_retriever(search_kwargs={\"k\": 6})  # Increased from 4 to 6\n",
    ")\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=compression_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Function to answer questions\n",
    "def answer_question(query):\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n",
    "\n",
    "# Example usage\n",
    "query = \"Can you summerize the topic?\"\n",
    "answer, source_docs = answer_question(query)\n",
    "print(query,\"\\n\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\nSource Documents:\")\n",
    "for doc in source_docs:\n",
    "    print(f\"- : {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 28bd0b85-5696-4ae2-b668-27ba0ac01195\n",
      "{'id_': '2b05c1a6-8fa3-4273-b168-e2bdabada1a6', 'embedding': None, 'metadata': {}, 'excluded_embed_metadata_keys': [], 'excluded_llm_metadata_keys': [], 'relationships': {}, 'text': '# Journal of Physics: Conference Series\\n\\nPAPER &#8226; OPEN ACCESS\\n\\nResearch on Machine Learning and Its Algorithms and Development\\n\\nTo cite this article: Wei Jin 2020 J. Phys.: Conf. Ser. 1544 012003\\n\\nView the article online for updates and enhancements.\\n\\nIOP ebooks\\n\\nBringing together innovative digital publishing with leading authors from the global scientific community.\\n\\nStart exploring the collection-download the first chapter of every title for free.\\n\\nThis content was downloaded from IP address 158.46.154.149 on 03/06/2020 at 13:35', 'mimetype': 'text/plain', 'start_char_idx': None, 'end_char_idx': None, 'text_template': '{metadata_str}\\n\\n{content}', 'metadata_template': '{key}: {value}', 'metadata_seperator': '\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Extract documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "documents = extract_documents(path)\n",
    "\n",
    "# Inspect the first document to understand its structure\n",
    "first_document = documents[0]\n",
    "print(first_document.__dict__)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
