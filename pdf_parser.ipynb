{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better data parser is essential as there are different types of things to be parsed like images,charts,diagrams etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "parser_key = os.getenv(\"llama_parser\")#It only parse 1000 pages per day for free or you will have to upgrade the plan ðŸ¤‘ðŸ’¸\n",
    "'''https://cloud.llamaindex.ai/api-key'''#and there must be 1200 pages per file max\n",
    "pinecone_key=os.getenv(\"pinecone_api_key\")\n",
    "if parser_key:\n",
    "    print(True)\n",
    "if pinecone_key:\n",
    "    print(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id cac11eca-232e-4dc8-bea0-7284f10dfb93\n",
      "# Journal of Physics: Conference Series\n",
      "\n",
      "PAPER &#8226; OPEN ACCESS\n",
      "\n",
      "Research on Machine Learning and Its Algorithms and Development\n",
      "\n",
      "To cite this article: Wei Jin 2020 J. Phys.: Conf. Ser. 1544 012003\n",
      "\n",
      "View the article online for updates and enhancements.\n",
      "\n",
      "IOP ebooks\n",
      "\n",
      "Bringing together innovative digital publishing with leading authors from the global scientific community.\n",
      "\n",
      "Start exploring the collection-download the first chapter of every title for free.\n",
      "\n",
      "This content was downloaded from IP address 158.46.154.149 on 03/06/2020 at 13:35\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "# Research on Machine Learning and Its Algorithms and Development\n",
      "\n",
      "Wei Jin\n",
      "Northwestern Polytechnical University Ming De College, Xiâ€™an, Shaanxi, China\n",
      "\n",
      "Abstract: This article analyzes the basic classification of machine learning, including supervised learning, unsupervised learning, and reinforcement learning. It combines analysis on common algorithms in machine learning, such as decision tree algorithm, random forest algorithm, artificial neural network algorithm, SVM algorithm, Boosting and Bagging algorithm, BP algorithm. Through the development of theoretical systems, further improvement of autonomous learning capabilities, the integration of multiple digital technologies, and the promotion of personalized custom services, the purpose is to improve people's awareness of machine learning and accelerate the speed of popularization of machine learning.\n",
      "\n",
      "# 1. Intro\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "# ICSP 2020\n",
      "\n",
      "# Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "\n",
    "# Apply nest_asyncio to handle asynchronous operations\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize LlamaParse with API key from environment variable\n",
    "api_key = parser_key  # Ensure your API key is set here or in the environment variables\n",
    "parser = LlamaParse(\n",
    "    api_key=api_key,\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Function to handle loading of documents synchronously\n",
    "def extract_documents_sync(path):\n",
    "    if os.path.isfile(path):\n",
    "        return parser.load_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return parser.load_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Function to handle loading of documents asynchronously\n",
    "async def extract_documents_async(path):\n",
    "    if os.path.isfile(path):\n",
    "        return await parser.aload_data(path)\n",
    "    elif os.path.isdir(path):\n",
    "        files = [os.path.join(path, filename) for filename in os.listdir(path)]\n",
    "        return await parser.aload_data(files)\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is neither a file nor a directory.\")\n",
    "\n",
    "# Path to the file or directory containing the documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "\n",
    "# Extract and print the content of the documents synchronously\n",
    "documents = extract_documents_sync(path)\n",
    "for doc in documents:\n",
    "    print(doc.text[:1000])# Print the first 1000 characters of each \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk size plays a crucial part determining the accuracy of the model as it ha slimited context window. We also keep in mind the \"lost in the middle problem\" while deciding the chunk size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned chunk has mixed level of relevance so the most relevant documents are not necessarily always at the top (top_k=5); As it is spreaded accross the chunks we need to rerank them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also using hybrid search (keyword search and vector search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pinecone, ServerlessSpec\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize Pinecone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Dict, List, Literal, Optional, Tuple, Type, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_key)#https://app.pinecone.io/organizations/-O2_C4SJIoSm2oxNIp2Z/projects/dbbe75bb-5398-4cf9-9a9e-719985dac1d3/indexes\n",
    "index_name = \"docs-metadata\"\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "    region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "    spec=ServerlessSpec(cloud=cloud,region=region)\n",
    "    # pc.create_index(name=index_name, dimension=768, metric=\"cosine\", spec=spec)\n",
    "    \n",
    "    \n",
    "\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be ready\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')#opensource encoder\n",
    "\n",
    "# Function to embed and store text chunks\n",
    "def embed_and_store(chunks):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = model.encode(chunk)\n",
    "        index.upsert(vectors=[{\"id\": f\"chunk_{i}\", \"values\": embedding.tolist(), \"metadata\": {\"text\": chunk}}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [doc.text for doc in documents]\n",
    "embed_and_store(chunks)\n",
    "\n",
    "# Hybrid search function\n",
    "def hybrid_search(query, top_k=5):\n",
    "    query_embedding = model.encode(query)\n",
    "    results = index.query(\n",
    "        vector=query_embedding.tolist(),\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import transformers\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaConfig\n",
    "# from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# HUGGINGFACE_TOKEN= \"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\"\n",
    "\n",
    "# # Load and modify the LLaMA configuration\n",
    "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# config = LlamaConfig.from_pretrained(model_id, use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "# # Disable chunked prefill and set max model length\n",
    "# config.use_cache = True\n",
    "# config.max_position_embeddings = 4096  # Adjust this value as needed\n",
    "\n",
    "# # Create the model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     config=config,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# )\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Create a text generation pipeline\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    "# )\n",
    "\n",
    "# # Initialize the reranker\n",
    "# reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# # Hybrid search function\n",
    "# def hybrid_search(query, top_k=20):\n",
    "#     results = index.query(\n",
    "#         vector=query,\n",
    "#         top_k=top_k,\n",
    "#         include_metadata=True\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "# # Rerank function\n",
    "# def rerank(results, query):\n",
    "#     texts = [result.metadata['text'] for result in results.matches]\n",
    "#     pairs = [[query, text] for text in texts]\n",
    "#     scores = reranker.predict(pairs)\n",
    "    \n",
    "#     reranked_results = sorted(zip(results.matches, scores), key=lambda x: x[1], reverse=True)\n",
    "#     return [item[0] for item in reranked_results]\n",
    "\n",
    "# # Function to generate answer\n",
    "# def generate_answer(query, context):\n",
    "#     prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "#     outputs = pipeline(\n",
    "#         prompt,\n",
    "#         max_new_tokens=256,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.7,\n",
    "#         no_repeat_ngram_size=3,\n",
    "#     )\n",
    "#     return outputs[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# # Main function to answer queries\n",
    "# def answer_query(query):\n",
    "#     search_results = hybrid_search(query)\n",
    "#     reranked_results = rerank(search_results, query)\n",
    "#     context = \" \".join([result.metadata[\"text\"] for result in reranked_results[:5]])  # Use top 5 reranked results\n",
    "#     answer = generate_answer(query, context)\n",
    "#     return answer\n",
    "\n",
    "# # Example usage\n",
    "# query = \"What are the main algorithms used in machine learning?\"\n",
    "# answer = answer_query(query)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TypeBaseModel' from 'langchain_core.utils.pydantic' (c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\utils\\pydantic.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, pipeline\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\prompts\\__init__.py:47\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfew_shot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     FewShotChatMessagePromptTemplate,\n\u001b[0;32m     44\u001b[0m     FewShotPromptTemplate,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfew_shot_with_templates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FewShotPromptWithTemplates\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_prompt\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PipelinePromptTemplate\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\prompts\\loading.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Dict, Optional, Union\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasePromptTemplate\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\output_parsers\\__init__.py:28\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonOutputParser, SimpleJsonOutputParser\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     CommaSeparatedListOutputParser,\n\u001b[0;32m     24\u001b[0m     ListOutputParser,\n\u001b[0;32m     25\u001b[0m     MarkdownListOutputParser,\n\u001b[0;32m     26\u001b[0m     NumberedListOutputParser,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     JsonOutputKeyToolsParser,\n\u001b[0;32m     30\u001b[0m     JsonOutputToolsParser,\n\u001b[0;32m     31\u001b[0m     PydanticToolsParser,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticOutputParser\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\output_parsers\\openai_tools.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValidationError\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_partial_json\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeBaseModel\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tool_call\u001b[39m(\n\u001b[0;32m     22\u001b[0m     raw_tool_call: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     return_id: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     27\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a single tool call.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        OutputParserException: If the tool call is not valid JSON.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TypeBaseModel' from 'langchain_core.utils.pydantic' (c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\utils\\pydantic.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ... (your existing Pinecone and SentenceTransformer setup code)\n",
    "\n",
    "# Load Mistral model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up pipeline\n",
    "response_generation_pipeline = pipeline(\n",
    "    model=mistral_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "# Create LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=response_generation_pipeline)\n",
    "\n",
    "# Set up embeddings for LangChain\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Create LangChain vectorstore\n",
    "vectorstore = LangchainPinecone(index, embeddings.embed_query, \"text\")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create prompt template\n",
    "system_prompt = \"\"\"Use the given context to answer the question. \n",
    "If you don't know the answer, say you don't know. \n",
    "Use three sentences maximum and keep the answer concise. \n",
    "Context: {context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create question-answer chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create retrieval chain\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Function to answer questions\n",
    "def answer_question(question):\n",
    "    result = chain.invoke({\"input\": question})\n",
    "    return result[\"answer\"], result.get(\"source_documents\", [])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you've already embedded and stored your documents\n",
    "    question = \"What is the capital of France?\"\n",
    "    answer, sources = answer_question(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for i, doc in enumerate(sources):\n",
    "        print(f\"Source {i + 1}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import time\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "index_name = \"docs-metadata\"\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "    region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "    spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "    \n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"HUGGINGFACE_TOKEN\"\n",
    "model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize SentenceTransformer for embedding\n",
    "model_llm = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Initialize CrossEncoder for reranking\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Initialize Mistral LLM\n",
    "chatbot = pipeline(\"text-generation\", model=model_llm, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Function to embed and store text chunks\n",
    "def embed_and_store(chunks, chunk_size=512):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Split long chunks\n",
    "        sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "        for j, sub_chunk in enumerate(sub_chunks):\n",
    "            embedding = model.encode(sub_chunk)\n",
    "            index.upsert(vectors=[{\"id\": f\"chunk_{i}_{j}\", \"values\": embedding.tolist(), \"metadata\": {\"text\": sub_chunk}}])\n",
    "\n",
    "# Assuming 'documents' is already defined\n",
    "chunks = [doc.text for doc in documents]\n",
    "embed_and_store(chunks)\n",
    "\n",
    "# Hybrid search function with reranking\n",
    "def hybrid_search(query, top_k=10):\n",
    "    query_embedding = model.encode(query)\n",
    "    results = index.query(\n",
    "        vector=query_embedding.tolist(),\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Rerank the results\n",
    "    texts = [match.metadata['text'] for match in results.matches]\n",
    "    pairs = [[query, text] for text in texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    reranked_results = sorted(zip(results.matches, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [item[0] for item in reranked_results]\n",
    "\n",
    "# Function to generate response using Mistral LLM\n",
    "def generate_response(query, context):\n",
    "    prompt = f\"Based on the following context, answer the question:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Provide accurate and concise answers based on the given context.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = chatbot(messages, max_length=512, num_return_sequences=1)\n",
    "    return response[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Main function to answer queries\n",
    "def answer_query(query):\n",
    "    search_results = hybrid_search(query)\n",
    "    context = \" \".join([result.metadata[\"text\"] for result in search_results[:3]])  # Use top 3 reranked results\n",
    "    answer = generate_response(query, context)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "query = \"What are the main algorithms used in machine learning?\"\n",
    "answer = answer_query(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'field_validator' from 'pydantic' (c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer  \u001b[38;5;66;03m# pip install mistral_inference\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmistral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralTokenizer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstruct\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserMessage\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstruct\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatCompletionRequest\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistral_common\\tokens\\tokenizers\\mistral.py:9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Dict, Generic, List, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     TokenizerException,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstruct\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     UATS,\n\u001b[0;32m     11\u001b[0m     AssistantMessageType,\n\u001b[0;32m     12\u001b[0m     SystemMessageType,\n\u001b[0;32m     13\u001b[0m     ToolMessageType,\n\u001b[0;32m     14\u001b[0m     UserMessageType,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstruct\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InstructRequestNormalizer\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstruct\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatCompletionRequest\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistral_common\\protocol\\instruct\\messages.py:11\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# compatibility with 3.8\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     Annotated,\n\u001b[0;32m      7\u001b[0m     TypeAlias,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralBase\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstruct\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtool_calls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolCall\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRoles\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m, Enum):\n\u001b[0;32m     15\u001b[0m     system \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mistral_common\\protocol\\instruct\\tool_calls.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, TypeVar, Union\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m field_validator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistral_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralBase\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFunction\u001b[39;00m(MistralBase):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'field_validator' from 'pydantic' (c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\__init__.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id cac11eca-476f-4930-a4e5-9aed95f347f5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d49c8c28864ad9820ef2d73573b402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sayan Maity\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-mpnet-base-dot-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "# Journal of Physics: Conference Series\n",
      "\n",
      "PAPER &#8226; OPEN ACCESS\n",
      "\n",
      "Research on **MACHINE** **LEARNING** and Its **ALGORITHMS** and Development\n",
      "\n",
      "To cite this article: Wei Jin 2020 J. Phys.: Conf. Ser. 1544 012003\n",
      "\n",
      "View **THE** article online for updates and enhancements.\n",
      "\n",
      "IOP ebooks\n",
      "\n",
      "Bringing toge**THE**r innovative digital publishing with leading authors from **THE** global scientific community.\n",
      "\n",
      "Start exploring **THE** collection-download **THE** first chapter of every title for free.\n",
      "\n",
      "This co\n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "ience for **THE** economic development of **THE** industry.\n",
      "\n",
      "# 2. Basic Classification of **MACHINE** **LEARNING**\n",
      "\n",
      "# 2.1. Supervised **LEARNING**\n",
      "\n",
      "In **THE** process of **MACHINE** **LEARNING**, supervised **LEARNING** belongs to a relatively basic **LEARNING** method. This **LEARNING** method refers to **THE** establishment of corresponding **LEARNING** goals by people before **LEARNING**. During **THE** initial training of **THE** **MACHINE**, **THE** **MACHINE** relies on information technol\n",
      "\n",
      "\n",
      "Chunk 2:\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "# Research on **MACHINE** **LEARNING** and Its **ALGORITHMS** and Development\n",
      "\n",
      "Wei Jin\n",
      "Northwestern Polytechnical University Ming De College, Xiâ€™an, Shaanxi, China\n",
      "\n",
      "Abstract: This article analyzes **THE** basic classification of **MACHINE** **LEARNING**, including supervised **LEARNING**, unsupervised **LEARNING**, and reinforcement **LEARNING**. It combines analysis on commo\n",
      "\n",
      "\n",
      "Chunk 3:\n",
      "ologies, and **THE** promotion of personalized custom services, **THE** purpose is to improve people's aw**ARE**ness of **MACHINE** **LEARNING** and accelerate **THE** speed of popularization of **MACHINE** **LEARNING**.\n",
      "\n",
      "# 1. Introduction\n",
      "\n",
      "With **THE** rapid development of science and technology, artificial intelligence has also ushered in new development opportunities. **MACHINE** technology based on computer technology incorporates multidisciplinary **THE**oretical knowledge, such as statisti\n",
      "\n",
      "\n",
      "Chunk 4:\n",
      "ve some classification or regression problems, which is highly systematic. Currently, **THE** classic **LEARNING** methods commonly used include BN, SVN, KNN, etc. Because **THE** entire **LEARNING** process has purpose, **THE** **MACHINE** **LEARNING** process presents a certain regularity, and **THE** **LEARNING** content is more systematic [1].\n",
      "\n",
      "# 2.2. Unsupervised **LEARNING**\n",
      "\n",
      "Corresponding to supervised **LEARNING** is unsupervised **LEARNING**. **THE** so-called unsupervised **LEARNING** \n",
      "\n",
      "\n",
      "Chunk 5:\n",
      "e by IOP Publishing Ltd\n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "# ICSP 2020\n",
      "\n",
      "IOP Publishing\n",
      "\n",
      "Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "# ICSP 2020\n",
      "\n",
      "# Journal of Physics: Conference Series 1544 (2020) 012003 doi:10.1088/1742-6596/1544/1/012003\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "# Function to chunk text into smaller pieces\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# Function for keyword search using TF-IDF\n",
    "def keyword_search(chunks, query):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, tfidf_matrix)[0]\n",
    "    return similarities\n",
    "\n",
    "# Function for vector search using sentence embeddings\n",
    "def vector_search(chunks, query, model):\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "    query_embedding = model.encode([query])\n",
    "    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
    "    return similarities\n",
    "\n",
    "# Function to perform hybrid search and rerank chunks\n",
    "def hybrid_search(chunks, query, model, keyword_weight=0.5):\n",
    "    # Keyword search\n",
    "    keyword_scores = keyword_search(chunks, query)\n",
    "    \n",
    "    # Vector search\n",
    "    vector_scores = vector_search(chunks, query, model)\n",
    "    \n",
    "    # Combine scores\n",
    "    combined_scores = keyword_weight * keyword_scores + (1 - keyword_weight) * vector_scores\n",
    "    \n",
    "    # Sort chunks by combined score\n",
    "    ranked_chunks = sorted(zip(chunks, combined_scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [chunk for chunk, _ in ranked_chunks]\n",
    "\n",
    "# Function to extract keywords from query\n",
    "def extract_keywords(query):\n",
    "    # Simple keyword extraction (you might want to use a more sophisticated method)\n",
    "    return re.findall(r'\\w+', query.lower())\n",
    "\n",
    "# Path to the file or directory containing the documents\n",
    "path = r\"C:\\Users\\Sayan Maity\\OneDrive\\Desktop\\chat_pdf\\Research_on_Machine_Learning_and_Its_Algorithms_an.pdf\"\n",
    "\n",
    "# Extract the content of the documents synchronously\n",
    "documents = extract_documents_sync(path)\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')  # This is a smaller, faster model\n",
    "\n",
    "# Process each document\n",
    "for doc in documents:\n",
    "    # Chunk the document\n",
    "    chunks = chunk_text(doc.text)\n",
    "    \n",
    "    # Define your query\n",
    "    query = \"What are the main machine learning algorithms discussed?\"\n",
    "    \n",
    "    # Perform hybrid search and reranking\n",
    "    ranked_chunks = hybrid_search(chunks, query, model)\n",
    "    \n",
    "    # Extract keywords from the query\n",
    "    keywords = extract_keywords(query)\n",
    "    \n",
    "    # Print the top 5 chunks with highlighted keywords\n",
    "    for i, chunk in enumerate(ranked_chunks[:5]):\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        highlighted_chunk = chunk\n",
    "        for keyword in keywords:\n",
    "            highlighted_chunk = re.sub(f'(?i){keyword}', f'**{keyword.upper()}**', highlighted_chunk)\n",
    "        print(highlighted_chunk[:500])  # Print first 500 characters of each chunk\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use corrective rag agent for more elaborate correct result . It works like\n",
    "question->retrieve from doc->grade the doc(according to the relevance)-->(no irrelivance)generate answer-->else web search->answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sayan Maity\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from langchain_community.vectorstores import FAISS  # Updated import\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LlamaParse\n",
    "parser_key = os.getenv(\"llama_parser\")\n",
    "parser = LlamaParse(api_key=parser_key, result_type=\"markdown\", num_workers=4, verbose=True)\n",
    "\n",
    "# Global variables\n",
    "loaded_db = None\n",
    "qa_chain = None\n",
    "document_summary = \"\"\n",
    "\n",
    "def extract_documents(file):\n",
    "    return parser.load_data(file.name)\n",
    "\n",
    "def process_document(file):\n",
    "    global loaded_db, qa_chain, document_summary\n",
    "\n",
    "    # Extract documents\n",
    "    documents = extract_documents(file)\n",
    "\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    # Split documents into smaller chunks\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc.text)\n",
    "        texts.extend(chunks)\n",
    "        metadatas.extend([doc.metadata] * len(chunks))\n",
    "\n",
    "    # Initialize Hugging Face embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Create and save FAISS index\n",
    "    db = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "\n",
    "    # Remove old index files\n",
    "    if os.path.exists(\"faiss_index\"):\n",
    "        shutil.rmtree(\"faiss_index\")\n",
    "\n",
    "    # Save new index\n",
    "    db.save_local(\"faiss_index\")\n",
    "\n",
    "    # Load the saved index\n",
    "    loaded_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # Initialize Hugging Face pipeline for language model\n",
    "    model_name = \"google/flan-t5-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    # Create a more detailed prompt template\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Provide a detailed explanation in your answer, using up to five sentences.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Detailed Answer:\"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    # Set up retriever with contextual compression\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=loaded_db.as_retriever(search_kwargs={\"k\": 6})\n",
    "    )\n",
    "\n",
    "    # Create RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=compression_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "\n",
    "    # Generate document summary\n",
    "    summary_query = \"Provide a comprehensive summary of the entire document in about 500 words.\"\n",
    "    document_summary = qa_chain.invoke({\"query\": summary_query})[\"result\"]\n",
    "\n",
    "    return \"Document processed successfully. You can now ask questions or download the summary.\"\n",
    "\n",
    "def answer_question(query):\n",
    "    if not loaded_db or not qa_chain:\n",
    "        return \"Please process a document first.\"\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n",
    "def download_summary():\n",
    "    return document_summary\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=\"body {background-image: url('.img.jpeg'); background-size: cover; background-opacity: 0.5;}\") as demo:\n",
    "    gr.Markdown(\"# Document Q&A System\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document\")\n",
    "        process_button = gr.Button(\"Process Document\")\n",
    "    \n",
    "    status_output = gr.Textbox(label=\"Status\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Ask a question\")\n",
    "        answer_output = gr.Textbox(label=\"Answer\")\n",
    "    \n",
    "    ask_button = gr.Button(\"Ask\")\n",
    "    \n",
    "    summary_output = gr.Textbox(label=\"Document Summary\", lines=10)\n",
    "    summary_button = gr.Button(\"Generate Summary\")\n",
    "    \n",
    "    process_button.click(process_document, inputs=[file_input], outputs=[status_output])\n",
    "    ask_button.click(answer_question, inputs=[question_input], outputs=[answer_output])\n",
    "    summary_button.click(download_summary, outputs=[summary_output])\n",
    "\n",
    "demo.launch(share=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
